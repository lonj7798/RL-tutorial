{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDabn4X4IHdd",
    "outputId": "396b6036-2dde-4673-f1d1-671b6e4e848e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hHuQ9OhvIPvw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaewon/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tianshou as ts\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO91IP3JJhtK"
   },
   "source": [
    "## Previous way of using 'gym.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9GDJQCuEInht"
   },
   "outputs": [],
   "source": [
    "train_envs = gym.make('CartPole-v0')\n",
    "test_envs = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUMsXBh_Jtxe"
   },
   "source": [
    "## Tianshou support parallel sampling for every algorithm.\n",
    "there is four type for **Vectorized Environment Wrapper**\\\n",
    "- DummyVectorEnv\n",
    "- SubprocVectorEnv\n",
    "- ShemVectorEnv\n",
    "- RayVectorEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1GlyRdEKJhZ"
   },
   "source": [
    "### DummyVectorEnv\n",
    "make 10 numbers of train_envs and 100 of test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8264xrGDI6Kv"
   },
   "outputs": [],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zpmVLVK7JX68",
    "outputId": "c7fbe875-3e30-4426-86b8-c3318db05a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef seed(self, seed):\\n  np.random.seed(seed)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you use custom env, you need to set seed:\n",
    "\"\"\"\n",
    "def seed(self, seed):\n",
    "  np.random.seed(seed)\n",
    "\"\"\"\n",
    "# if you don't set the seed value, every env could have same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA3OJL1PKqRB"
   },
   "source": [
    "# Build the Network\n",
    "\n",
    "Tianshou every Network and Optimizer of Pytorch (input, output should follow Tianshou API)*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hCqK6UnuKky5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, state_shape, action_shape):\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace = True),\n",
    "        nn.Linear(128, 128), nn.ReLU(inplace = True),\n",
    "        nn.Linear(128, 128), nn.ReLU(inplace = True),\n",
    "        nn.Linear(128, np.prod(action_shape))\n",
    "    )\n",
    "\n",
    "  def forward(self, obs, state = None, info={}):\n",
    "    if not isinstance(obs, torch.Tensor):\n",
    "      obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "    batch = obs.shape[0]\n",
    "    logits = self.model(obs.view(batch, -1))\n",
    "    return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f6pm5LbVMNvZ"
   },
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbcAV9-hM82a"
   },
   "source": [
    "- input: Obervation obs\n",
    "\n",
    "- logits means raw output of every NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMJumzc8Nlnw"
   },
   "source": [
    "# Setup Policy\n",
    "In order to define **policy**, we will use initial variables, net, optims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z_0aTZRQMou2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-425994702e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQNPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimation_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_update_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ts' is not defined"
     ]
    }
   ],
   "source": [
    "policy = ts.policy.DQNPolicy(net, optim, discount_factor = 0.9, estimation_step = 3, target_update_freq = 320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCK-IGyNON1N"
   },
   "source": [
    "# Setup Collector\n",
    "Collector is main concept of Tianshou. Collector helps environments that have diferrent policy to interact each other. From each step, Colletor make policy act with limited step or episode, and Colletor save it at the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4cBTp6zeRajk"
   },
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise = True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISevkJb7O6_y"
   },
   "source": [
    "# Train Policy with a Trainer\n",
    "\n",
    "Tianshou has tree trainer: **onpolicy_trainer(), offpolicy_trainer(), offline_trainer()**\\\n",
    "Trainer ends the train when the ploicy of test collector reach to the stop condition (stop_fn). (DQN is off-policy algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb2hdw3QPL5T",
    "outputId": "439ab4be-c57b-47bd-a584-7b524f8c60f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  71%|#######   | 7060/10000 [00:06<00:02, 1121.28it/s, env_step=7060, len=200, n/ep=1, n/st=10, rew=200.00]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! Use 6.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector,\n",
    "    max_epoch=10, step_per_epoch=10000, step_per_collect=10,\n",
    "    update_per_step=0.1, episode_per_test=100, batch_size=64,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ea5gcovSN6-"
   },
   "source": [
    "- max_epoch: maximum number of training epoch. train could be ended before the max_epoch\n",
    "- step_per_peoch: number of transition per epoch\n",
    "- step_per_collect: the number of transition when network is updated\n",
    "- episode_per_test: number of episode to vlaue the policy\n",
    "- batch_size: the size of sampling bath to train\n",
    "- train_fn: set the train env based on the current epoch and step index\n",
    "- test_fn: set the test env\n",
    "- stop_fn: return the bool type. reveices the value of non-discounted data\n",
    "- logger: save the trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrx-Q-TSQ6NT",
    "outputId": "4aaca326-f9bb-4f75-f39d-1e8e22b796b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaewon/anaconda3/lib/python3.8/site-packages/tianshou/utils/logger/tensorboard.py:84: UserWarning: Deprecated soon: BasicLogger has renamed to TensorboardLogger in #427.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import BasicLogger\n",
    "\n",
    "writer = SummaryWriter('log/dqn')\n",
    "logger = BasicLogger(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH93QNPXV2kt"
   },
   "source": [
    "# Save/Load Policy\n",
    "\n",
    "Saving and Loading is sas same as torch module as policy inherits torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YR0LmKMDWKXe",
    "outputId": "876c32c0-3d3d-40a3-e4e5-c20ecb3836e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9Bs8jA1tJ16"
   },
   "source": [
    "# Watch the Agent's Performance\n",
    "\n",
    "Collector support rendering. (35 FPS code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "X8nlDiszs7F2",
    "outputId": "5b70c9a8-3f63-4f58-968c-15cdb152a6a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaewon/anaconda3/lib/python3.8/site-packages/tianshou/data/collector.py:66: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 200,\n",
       " 'rews': array([200.]),\n",
       " 'lens': array([200]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 200.0,\n",
       " 'len': 200.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.07)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Policy with Customized Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! Test mean returns: 198.92\n"
     ]
    }
   ],
   "source": [
    "train_collector.collect(n_step=5000, random=True)\n",
    "\n",
    "policy.set_eps(0.1)\n",
    "for i in range(int(1e6)):  # total step\n",
    "    collect_result = train_collector.collect(n_step=10)\n",
    "\n",
    "    # once if the collected episodes' mean returns reach the threshold,\n",
    "    # or every 1000 steps, we test it on test_collector\n",
    "    if collect_result['rews'].mean() >= env.spec.reward_threshold or i % 1000 == 0:\n",
    "        policy.set_eps(0.05)\n",
    "        result = test_collector.collect(n_episode=100)\n",
    "        if result['rews'].mean() >= env.spec.reward_threshold:\n",
    "            print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "            break\n",
    "        else:\n",
    "            # back to training eps\n",
    "            policy.set_eps(0.1)\n",
    "\n",
    "    # train policy with a sampled batch data from buffer\n",
    "    losses = policy.update(64, train_collector.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tianshou Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
