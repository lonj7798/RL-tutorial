{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0c12cb",
   "metadata": {},
   "source": [
    "``` python\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=\n",
    "                        (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        ...\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        ...\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c309f9",
   "metadata": {},
   "source": [
    "- `action_space`: contain all of the actions of agent\n",
    "- `observation_space`: all data of the environment observed by agent\n",
    "- `reset`: reset env to an initial state\n",
    "- `step`: one time step\n",
    "- `render`: print rendition of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b1bc2",
   "metadata": {},
   "source": [
    "## Stock Trading Env\n",
    "\n",
    "The model(agent) observes the price of the stock before making an action(trade)\n",
    "\n",
    "`observation_space`contains most of input variables, such as open price, high, low, close, daily volume, and the agent observe those variables \n",
    "\n",
    "each `step`, the agent cosider the action up to the current and past price of the stock\n",
    "\n",
    "`action_space` has three actions: buy, sell, hold\n",
    "\n",
    "need to know the amount of a given stock to buy or sell each step(time). Using `Box` space, create action space that has a discrete number of action types (buy, sell, hold), or a continuous psectrum of amounts, such as 'buy x%, sell y%, hold z%'\n",
    "\n",
    "`reward` is the account balance multiplied by some fraction of the number of time steps. Because of the delay reward is that the we wants that the agent consider the long-term investigation than short-term trade. (단타 보다는 장타...??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb83e73",
   "metadata": {},
   "source": [
    "### Define action_space & observatrion_step\n",
    "\n",
    "`pandas` data frame passes the stock price to `environment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6076db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f420ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ACCOUNT_BALANCE = 2147483647\n",
    "MAX_NUM_SHARES = 2147483647\n",
    "MAX_SHARE_PRICE = 5000\n",
    "MAX_OPEN_POSITIONS = 5\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 10000\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']} ### what is metadata / what kinds of render.modes exist?\n",
    "    \n",
    "    def __init__(self, df): # input is dataframe of pandas\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "        \n",
    "        # Continuous action: buy x%, sell y%, hold z%\n",
    "        ### How to set variables of the Box\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float16)\n",
    "        \n",
    "        # prices contains the candle chart(OHLC) values for the last 5 days\n",
    "        ### each step is 1 day\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "        \n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        ### current_step = day passed.\n",
    "        ### from df(stock df) read current step to after 5 steps stcok data(Open, High, Low, Close, Volume)\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'High'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'Volume'].values / MAX_NUM_SHARES,\n",
    "        ])\n",
    "        \n",
    "        # additional data and scale each value to between 0-1\n",
    "        ### check the variables!!!!!!!!!!\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        # stock price is random value between Open and Close\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, \"Open\"], self.df.loc[self.current_step, \"Close\"])\n",
    "        \n",
    "        ### what is the action list?\n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "        \n",
    "        if action_type < 1:\n",
    "            # Buy amount % of balance in shares\n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (\n",
    "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action_type < 2:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        # set the net_worth\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "            \n",
    "    # step includes action by agent\n",
    "    def step(self, action):\n",
    "        # take action\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # add current_step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # we have limited df. so we need to check the end of the training.\n",
    "        if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n",
    "            self.current_step = 0\n",
    "            \n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(\n",
    "            0, len(self.df.loc[:, 'Open'].values) - 6)\n",
    "\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60fd2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d3b0f1111ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStockTradingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Env\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# We need to manually parse the ID so we can check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# the version without error-ing out in self.spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_env_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# Get all versions of this spec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mparse_env_id\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mto\u001b[0m \u001b[0minclude\u001b[0m \u001b[0man\u001b[0m \u001b[0moptional\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV_ID_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         raise error.Error(\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "env = gym.make([lambda: StockTradingEnv(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2900eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aff1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"aapl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c9a62",
   "metadata": {},
   "source": [
    "### DummyVectorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c857ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaewon/anaconda3/lib/python3.8/site-packages/gym/spaces/box.py:78: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: StockTradingEnv(df) for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: StockTradingEnv(df) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff7d8f",
   "metadata": {},
   "source": [
    "### Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5dfce94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b7a91e19ece5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maction_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = ts.utils.net.common.Net(state_shape, action_shape, activation='ReLu')\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24583bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "train_envs.seed(args.seed)\n",
    "test_envs.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f829344",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4b4e19c49103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import json\n",
    "import datetime as dt\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('aapl.csv')\n",
    "df = df.sort_values('Date')\n",
    "# The algorithms require a vectorized environment to run\n",
    "env = DummyVecEnv([lambda: StockTradingEnv(df)])\n",
    "\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "obs = env.reset()\n",
    "for i in range(2000):\n",
    "  action, _states = model.predict(obs)\n",
    "  obs, rewards, done, info = env.step(action)\n",
    "  env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a598642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = gym.make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca19173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
